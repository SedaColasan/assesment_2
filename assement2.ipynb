{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12985821",
   "metadata": {},
   "source": [
    "# Let go begin to add some library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c1fb1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #we add pandas library\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings # supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905497ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"EBR_Building_Permits.csv\") #we add our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10) #we see 10 data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed1dbd",
   "metadata": {},
   "source": [
    "first we need learn dataset columns name and data types because of how we can use this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns #columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb51531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes #we see data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0715f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() #We learn detailed information about our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fd77b",
   "metadata": {},
   "source": [
    "just we need SQUARE FOOTAGE, PROJECT VALUE and PERMIT FEE columns because of ZIP, LATITUDE, and LONGITUDE columns for locations also INTERNAL ID is like a column ID number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9807cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() #The statistics of the numeric numbers in the data set were written to the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac97b5",
   "metadata": {},
   "source": [
    "We will now provide the missing data. Why do we check for missing data? To obtain the data, improve accuracy in statistical analysis, improve the performance of machine learning models, and perform other operations on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be1a20",
   "metadata": {},
   "source": [
    "# EDA Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7310bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we gonna check missing values\n",
    "\n",
    "missing_value=[\"n.a\",\"?\",\"NA\",\"n/a\",\"na\",\"--\",\" \"]\n",
    "data=pd.read_csv(\"EBR_Building_Permits.csv\", na_values=missing_value)\n",
    "\n",
    "def make_int(i):\n",
    "    try:\n",
    "        return int(i)\n",
    "    except:\n",
    "        return pd.np.nan\n",
    "    \n",
    "# apply make_int function to the entire series using map\n",
    "data['PROJECT VALUE'] = data['PROJECT VALUE'].map(make_int)\n",
    "data['PROJECT VALUE'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e76e3",
   "metadata": {},
   "source": [
    "Total number of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ed2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum() #nan values are maked true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_null = data.isnull().sum().sort_values()\n",
    "cols_with_null.head(25) #The column with the most missing data came last."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14967f",
   "metadata": {},
   "source": [
    "We use find because the fillna() function allows you to fill in missing items with a specific value or according to a specific strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GEOLOCATION'].fillna(method='bfill') #We print the missing values using a specific value using fillna() ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7fb0e",
   "metadata": {},
   "source": [
    "Missing values ​​in the column are filled with the median value. The median value is written instead of None, which are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LONGITUDE'].fillna(data['LONGITUDE'].median()) #Missing values Instead of None, we write the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aef56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[684] #We see the data at position 684."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dd951",
   "metadata": {},
   "source": [
    "We select the CONTRACTOR ADDRESS column. \n",
    "Next, we remove the leading commas from each value with the str.strip(',') expression. \n",
    "The result is obtained as ordered_value, which is the ordered Series.\n",
    "If you want to completely delete a string, you can use the replace() method. \n",
    "Our goal here is that we want the value to look clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duzenlenmis_deger = data['CONTRACTOR ADDRESS'].str.replace(',', '')\n",
    "#In this code, we remove all commas from each value with the expression str.replace(',', '').\n",
    "print(duzenlenmis_deger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duzenlenmis_deger.head(20) #we look new "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae417d1",
   "metadata": {},
   "source": [
    "we replace the duzenlenmis_deger Series with the CONTRACTOR ADDRESS column and print the data frame data to display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CONTRACTOR ADDRESS'] = duzenlenmis_deger \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "sample_data = len(data['PERMIT NUMBER'])\n",
    "# Using the sample mean to estimate the population mean\n",
    "sample_mean = np.mean(sample_data)\n",
    "# Using the sample standard deviation to estimate the population standard deviation\n",
    "sample_std = np.std(sample_data)\n",
    "# Determine the population size\n",
    "population_size = 1000\n",
    "# Confidence interval calculation\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # z-score for 0.95 confidence level\n",
    "margin_of_error = z_score * sample_std\n",
    "lower_limit = sample_mean - margin_of_error\n",
    "upper_limit = sample_mean + margin_of_error\n",
    "# Print the results\n",
    "print(\"Sample Mean:\", sample_mean)\n",
    "print(\"Sample Standard Deviation:\", sample_std)\n",
    "print(\"Confidence Interval:\", (lower_limit, upper_limit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "sample_mean = np.mean(sample_data)\n",
    "margin_of_error = z_score * sample_std\n",
    "# make graph\n",
    "fig, ax = plt.subplots()\n",
    "# Adding the sample mean as a line\n",
    "ax.axhline(y=sample_mean, color='blue', linestyle='-', label='Sample Average')\n",
    "# Adding error bars\n",
    "ax.errorbar(0, sample_mean, yerr=margin_of_error, color='red', fmt='o', capsize=5, label='Confidence Interval')\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Sample Mean and Confidence Interval')\n",
    "# Show axis zero lines\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "# Define the boundaries of the axes\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(sample_mean - 2 * margin_of_error, sample_mean + 2 * margin_of_error)\n",
    "# Rotate axis labels\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "# Show graphics\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023343cd",
   "metadata": {},
   "source": [
    "We determine values such as (sample_size), number of apartment buildings in our dataset (apartment_buildings) and entrance dimensions (population_size). We calculate (sample_proportion).\n",
    "\n",
    "Next, we calculate the z-score (z_score) for a given confidence level (confidence_level). In this example, we used the scipy.stats module of the SciPy library to calculate the z-score using stats.norm.ppf() cells.\n",
    "\n",
    "To calculate the confidence interval, we determine the margin of error (margin_of_error) value and the lower and upper limits (lower_limit and upper_limit).\n",
    "\n",
    "Using this code, we can calculate the annual population measurement confidence interval for apartment buildings in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size and number of apartment buildings in the sample\n",
    "sample_size = len(data['PERMIT NUMBER'])\n",
    "apartment_buildings = len(data['PERMIT NUMBER'])\n",
    "# Population size (a large number is selected by default)\n",
    "population_size = 10000\n",
    "# Sample rate\n",
    "sample_proportion = apartment_buildings / sample_size\n",
    "# Z-score (for a certain level of confidence)\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "# Calculation of confidence interval\n",
    "margin_of_error = z_score * np.sqrt((sample_proportion * (1 - sample_proportion)) / sample_size)\n",
    "lower_limit = sample_proportion - margin_of_error\n",
    "upper_limit = sample_proportion + margin_of_error\n",
    "# Print the results\n",
    "print(\"Confidence Interval:\", (lower_limit, upper_limit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65d48f",
   "metadata": {},
   "source": [
    "creates a graph showing the error bar and the midpoint. We add lines and error bar to the chart with plt.plot() and ax.errorbar() commands. We specify axis labels, title and appropriate graphic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval and midpoint\n",
    "confidence_interval = (lower_limit, upper_limit)\n",
    "mean_point = (lower_limit + upper_limit) / 2\n",
    "# Length of error bar\n",
    "error = (upper_limit - lower_limit) / 2\n",
    "# Graph creation\n",
    "fig, ax = plt.subplots()\n",
    "# Show the mean point\n",
    "ax.plot(mean_point, 0, 'ro', label='Average Point')\n",
    "# Don't show the error bar\n",
    "ax.errorbar(mean_point, 0, xerr=error, fmt='o', color='blue', capsize=5, label='Confidence Interval')\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Values')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('Confidence Interval')\n",
    "# Show axis zero lines\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "# Graphics settings\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('none')\n",
    "# Show graphics\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace70b87",
   "metadata": {},
   "source": [
    "## Data for Ireland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42013",
   "metadata": {},
   "source": [
    "We will now combine the common columns in our two datasets to arrive at our data for Ireland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read first CSV file\n",
    "df1 = pd.read_csv('County_Statistics_Historic_Data.csv')\n",
    "\n",
    "# Read second CSV file\n",
    "df2 = pd.read_csv('BHQ09.20230524005704.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce393ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge operation\n",
    "merged_df = pd.merge(df1, df2, on='CountyName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879524e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(5) #we see 5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes #we see data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe() #The statistics of the numeric numbers in the data set were written to the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea198c",
   "metadata": {},
   "source": [
    "we have too many fields to delete in our dataset because we want a clean dataset. These are the parts that won't work for us. These are in order;\n",
    "OBJECTID, ORIGID, TimeStamp, IGEasting, IGNorthing, UGI, ConfirmedCovidCases, PopulationProportionCovidCases, ConfirmedCovidDeaths, ConfirmedCovidRecovered, SHAPE_Length, SHAPE_Area, STATISTIC, Statistic Label and C02074V02506."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['OBJECTID', 'ORIGID', 'TimeStamp', 'IGEasting', 'IGNorthing', 'UGI', 'ConfirmedCovidCases', 'PopulationProportionCovidCases', 'ConfirmedCovidDeaths', 'ConfirmedCovidRecovered', 'SHAPE_Length', 'SHAPE_Area', 'STATISTIC', 'Statistic Label', 'C02074V02506', 'TLIST(Q1)'] \n",
    "#we dont need this columns, we drop there\n",
    "merged_df.drop(to_drop,inplace=True, axis=1) #columns are permanently removed.\n",
    "merged_df.head(10) #we see 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48754480",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[25] #We display the data on line 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[30: 'Quarter'].head(10) #Selects data up to column \"Quarter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb593eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year and quarter\n",
    "merged_df['Year'] = merged_df['Quarter'].str[:4]\n",
    "merged_df['Quarter'] = merged_df['Quarter'].str[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(10) #we see 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1069a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we change the title names\n",
    "newData = {'CountyName':'County','PopulationCensus16':'Population','TLIST(Q1)':'Year','UNIT':'Square Meters','VALUE':'Price','Year':'Year'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns=newData, inplace=True) #renames columns\n",
    "merged_df.head(10) #we see 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0f8b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.dtypes #we see data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb623ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Population average calculation\n",
    "population_mean = np.mean(merged_df['Population'])\n",
    "\n",
    "# Population standard deviation calculation\n",
    "population_std = np.std(merged_df['Population'])\n",
    "\n",
    "# Calculation of the confidence interval (for example, 95% confidence interval)\n",
    "confidence_interval = stats.norm.interval(0.95, loc=population_mean, scale=population_std)\n",
    "\n",
    "# Print the results\n",
    "print(\"Population Mean:\", population_mean)\n",
    "print(\"Confidence Interval (95%):\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population data\n",
    "population_data = merged_df['Population']\n",
    "\n",
    "# Population average calculation\n",
    "population_mean = np.mean(population_data)\n",
    "\n",
    "# Population standard deviation calculation\n",
    "population_std = np.std(population_data)\n",
    "\n",
    "# Confidence interval calculation\n",
    "confidence_interval = stats.norm.interval(0.95, loc=population_mean, scale=population_std)\n",
    "\n",
    "# Histogram drawing\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(population_data, bins=20, color='skyblue', alpha=0.75)\n",
    "\n",
    "# mean line\n",
    "plt.axvline(population_mean, color='red', linestyle='--', linewidth=2, label='Average')\n",
    "\n",
    "# Confidence interval display\n",
    "plt.axvline(confidence_interval[0], color='orange', linestyle=':', linewidth=2, label='Confidence Interval (95%)')\n",
    "plt.axvline(confidence_interval[1], color='orange', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Population Distribution and Average and Confidence Interval')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e710775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yılları bir listeye dönüştürme\n",
    "years = merged_df['Year'].astype(int).unique().tolist()\n",
    "\n",
    "# Yıllara göre gruplama ve nüfus ortalamasını hesaplama\n",
    "population_mean_by_year = merged_df.groupby('Year')['Population'].mean().reindex(years).tolist()\n",
    "\n",
    "# Yıllara göre gruplama ve nüfus standart sapmasını hesaplama\n",
    "population_std_by_year = merged_df.groupby('Year')['Population'].std().reindex(years).tolist()\n",
    "\n",
    "# Yıllara göre gruplama ve veri sayısını hesaplama\n",
    "sample_size_by_year = merged_df.groupby('Year')['Population'].count().reindex(years).tolist()\n",
    "\n",
    "# Güven aralığı hesaplama\n",
    "confidence_interval = []\n",
    "for i in range(len(years)):\n",
    "    interval = stats.t.interval(0.95, sample_size_by_year[i] - 1, loc=population_mean_by_year[i], scale=population_std_by_year[i] / np.sqrt(sample_size_by_year[i]))\n",
    "    confidence_interval.append(interval)\n",
    "\n",
    "# Sonuçları bir veri çerçevesinde birleştirme\n",
    "result_df = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'Population Mean': population_mean_by_year,\n",
    "    'Sample Size': sample_size_by_year,\n",
    "    'Confidence Interval Lower': [interval[0] for interval in confidence_interval],\n",
    "    'Confidence Interval Upper': [interval[1] for interval in confidence_interval]\n",
    "})\n",
    "\n",
    "# Sonuçları ekrana yazdırma\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538b694",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b896e4",
   "metadata": {},
   "source": [
    "We're going to search for \"irish construction company\" using the Reddit API. \n",
    "Overhauling all comments about Irish construction company using information. \n",
    "It will return the latest 10 results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28740eb8",
   "metadata": {},
   "source": [
    "We need to install the PRAW library before running the code (pip install praw ). \n",
    "The code will print each result with title, URL, rating and number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Reddit API credentials\n",
    "client_id = \"xdK49rGVixhP6FZOfa-30A\"\n",
    "client_secret = \"hha3uYuI1C87WWfYHuGIWnZw_4ddIA\"\n",
    "username = \"seda_cct\"\n",
    "password = \"Erdem1169\" # My Reddit account password\n",
    "user_agent = \"/u/seda_cct\"  # A user agent that ı customize\n",
    "\n",
    "# Creating the PRAW (Python Reddit API Wrapper) object\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Search parameters\n",
    "search_query = \"ireland construction company\"\n",
    "subreddit = reddit.subreddit(\"all\")  # I used \"all\" to search all subreddits\n",
    "\n",
    "# Search subreddit to get search results\n",
    "search_results = subreddit.search(search_query, limit=10)  # İstenilen sonuç sayısı\n",
    "\n",
    "# Looping results\n",
    "for submission in search_results:\n",
    "    print(\"Title: \", submission.title)\n",
    "    print(\"URL: \", submission.url)\n",
    "    print(\"Score: \", submission.score)\n",
    "    print(\"Number of Comments: \", submission.num_comments)\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Reddit API credentials\n",
    "client_id = \"xdK49rGVixhP6FZOfa-30A\"\n",
    "client_secret = \"hha3uYuI1C87WWfYHuGIWnZw_4ddIA\"\n",
    "username = \"seda_cct\"\n",
    "password = \"Erdem1169\"  # My Reddit account password\n",
    "user_agent = \"/u/seda_cct\"  # A user agent that I customize\n",
    "\n",
    "# Creating the PRAW (Python Reddit API Wrapper) object\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Search parameters\n",
    "search_query = \"ireland construction company\"\n",
    "subreddit = reddit.subreddit(\"all\")  # \"all\" to search all subreddits\n",
    "\n",
    "# Search subreddit to get search results\n",
    "search_results = subreddit.search(search_query, limit=10)  # Desired number of results\n",
    "\n",
    "# Creating an empty list to store texts for sentiment analysis\n",
    "texts = []\n",
    "\n",
    "# Looping results\n",
    "for submission in search_results:\n",
    "    texts.append(submission.title)  # Başlık metnini listeye ekleme\n",
    "\n",
    "   \n",
    "    # You can optionally use the comment texts for sentiment analysis.\n",
    "    # for comment in submission.comments:\n",
    "    # texts.append(comment.body) # Add comment text to list\n",
    "\n",
    "# Making sentiment analysis on texts\n",
    "for text in texts:\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "\n",
    "    print(\"Text: \", text)\n",
    "    print(\"Sentiment: \", sentiment)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4a7d6",
   "metadata": {},
   "source": [
    "# Dashboard Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def update_data():\n",
    "    # A function where data is updated\n",
    "    # Here you can get your data and assign the current values to the components\n",
    "    pass\n",
    "\n",
    "# Creating the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Dashboard\")\n",
    "\n",
    "# Creating components\n",
    "label1 = tk.Label(root, text=\"Data 1:\")\n",
    "label1.pack()\n",
    "\n",
    "value1 = tk.Label(root, text=\"0\")\n",
    "value1.pack()\n",
    "\n",
    "label2 = tk.Label(root, text=\"Data 2:\")\n",
    "label2.pack()\n",
    "\n",
    "value2 = tk.Label(root, text=\"0\")\n",
    "value2.pack()\n",
    "\n",
    "button = tk.Button(root, text=\"Update Data\", command=update_data)\n",
    "button.pack()\n",
    "\n",
    "# Starting the application loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c53f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "def update_data():\n",
    "    # A function where data is updated\n",
    "    # You can update the graphics\n",
    "    population_data = merged_df['Population']\n",
    "    population_mean = np.mean(population_data)\n",
    "    population_std = np.std(population_data)\n",
    "    confidence_interval = stats.norm.interval(0.95, loc=population_mean, scale=population_std)\n",
    "\n",
    "    # Histogram drawing\n",
    "    plt.clf()  # Clear existing graphics\n",
    "    plt.hist(population_data, bins=20, color='skyblue', alpha=0.75)\n",
    "    plt.axvline(population_mean, color='red', linestyle='--', linewidth=2, label='Average')\n",
    "    plt.axvline(confidence_interval[0], color='orange', linestyle=':', linewidth=2, label='Confidence Interval (95%)')\n",
    "    plt.axvline(confidence_interval[1], color='orange', linestyle=':', linewidth=2)\n",
    "    plt.xlabel('Population')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Population Distribution and Average and Confidence Interval')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # To show the changes on the chart\n",
    "    canvas.draw()\n",
    "\n",
    "# Creating the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Dashboard\")\n",
    "\n",
    "# Creating the matplotlib figure\n",
    "fig, ax = plt.subplots()\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.get_tk_widget().pack()\n",
    "\n",
    "# Creating components\n",
    "button = tk.Button(root, text=\"Update Data\", command=update_data)\n",
    "button.pack()\n",
    "\n",
    "# Starting the application loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0f20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
